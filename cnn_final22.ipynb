{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [4],\n",
       "       [1],\n",
       "       [9],\n",
       "       [2],\n",
       "       [1],\n",
       "       [3],\n",
       "       [1],\n",
       "       [4],\n",
       "       [3],\n",
       "       [5],\n",
       "       [3],\n",
       "       [6],\n",
       "       [1],\n",
       "       [7],\n",
       "       [2],\n",
       "       [8],\n",
       "       [6],\n",
       "       [9],\n",
       "       [4],\n",
       "       [0],\n",
       "       [9],\n",
       "       [1],\n",
       "       [1],\n",
       "       [2],\n",
       "       [4],\n",
       "       [3],\n",
       "       [2],\n",
       "       [7],\n",
       "       [3],\n",
       "       [8],\n",
       "       [6],\n",
       "       [9],\n",
       "       [0],\n",
       "       [5],\n",
       "       [6],\n",
       "       [0],\n",
       "       [7],\n",
       "       [6],\n",
       "       [1],\n",
       "       [8],\n",
       "       [7],\n",
       "       [9],\n",
       "       [3],\n",
       "       [9],\n",
       "       [8],\n",
       "       [5],\n",
       "       [9],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [7],\n",
       "       [4],\n",
       "       [9],\n",
       "       [8],\n",
       "       [0],\n",
       "       [9],\n",
       "       [4],\n",
       "       [1],\n",
       "       [4],\n",
       "       [4],\n",
       "       [6],\n",
       "       [0],\n",
       "       [4],\n",
       "       [5],\n",
       "       [6],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [7],\n",
       "       [1],\n",
       "       [6],\n",
       "       [3],\n",
       "       [0],\n",
       "       [2],\n",
       "       [1],\n",
       "       [1],\n",
       "       [7],\n",
       "       [9],\n",
       "       [0],\n",
       "       [2],\n",
       "       [6],\n",
       "       [7],\n",
       "       [8],\n",
       "       [3],\n",
       "       [9],\n",
       "       [0],\n",
       "       [4],\n",
       "       [6],\n",
       "       [7],\n",
       "       [4],\n",
       "       [6],\n",
       "       [8],\n",
       "       [0],\n",
       "       [7],\n",
       "       [8],\n",
       "       [3],\n",
       "       [1],\n",
       "       [5],\n",
       "       [7],\n",
       "       [1],\n",
       "       [7],\n",
       "       [1],\n",
       "       [1],\n",
       "       [6],\n",
       "       [3],\n",
       "       [0],\n",
       "       [2],\n",
       "       [9],\n",
       "       [3],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [4],\n",
       "       [9],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [0],\n",
       "       [2],\n",
       "       [7],\n",
       "       [1],\n",
       "       [8],\n",
       "       [6],\n",
       "       [4],\n",
       "       [1],\n",
       "       [6],\n",
       "       [3],\n",
       "       [4],\n",
       "       [5],\n",
       "       [9],\n",
       "       [1],\n",
       "       [3],\n",
       "       [3],\n",
       "       [8],\n",
       "       [5],\n",
       "       [4],\n",
       "       [7],\n",
       "       [7],\n",
       "       [4],\n",
       "       [2],\n",
       "       [8],\n",
       "       [5],\n",
       "       [8],\n",
       "       [6],\n",
       "       [7],\n",
       "       [3],\n",
       "       [4],\n",
       "       [6],\n",
       "       [1],\n",
       "       [9],\n",
       "       [9],\n",
       "       [6],\n",
       "       [0],\n",
       "       [3],\n",
       "       [7],\n",
       "       [2],\n",
       "       [8],\n",
       "       [2],\n",
       "       [9],\n",
       "       [4],\n",
       "       [4],\n",
       "       [6],\n",
       "       [4],\n",
       "       [9],\n",
       "       [7],\n",
       "       [0],\n",
       "       [9],\n",
       "       [2],\n",
       "       [9],\n",
       "       [5],\n",
       "       [1],\n",
       "       [5],\n",
       "       [9],\n",
       "       [1],\n",
       "       [2],\n",
       "       [3],\n",
       "       [2],\n",
       "       [3],\n",
       "       [5],\n",
       "       [9],\n",
       "       [1],\n",
       "       [7],\n",
       "       [6],\n",
       "       [2],\n",
       "       [8],\n",
       "       [2],\n",
       "       [2],\n",
       "       [5],\n",
       "       [0],\n",
       "       [7],\n",
       "       [4],\n",
       "       [9],\n",
       "       [7],\n",
       "       [8],\n",
       "       [3],\n",
       "       [2],\n",
       "       [1],\n",
       "       [1],\n",
       "       [8],\n",
       "       [3],\n",
       "       [6],\n",
       "       [1],\n",
       "       [0],\n",
       "       [3],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [7],\n",
       "       [2],\n",
       "       [7],\n",
       "       [3],\n",
       "       [0],\n",
       "       [4],\n",
       "       [6],\n",
       "       [5],\n",
       "       [2],\n",
       "       [6],\n",
       "       [4],\n",
       "       [7],\n",
       "       [1],\n",
       "       [8],\n",
       "       [9],\n",
       "       [9],\n",
       "       [3],\n",
       "       [0],\n",
       "       [7],\n",
       "       [1],\n",
       "       [0],\n",
       "       [2],\n",
       "       [0],\n",
       "       [3],\n",
       "       [5],\n",
       "       [4],\n",
       "       [6],\n",
       "       [5],\n",
       "       [8],\n",
       "       [6],\n",
       "       [3],\n",
       "       [7],\n",
       "       [5],\n",
       "       [8],\n",
       "       [0],\n",
       "       [9],\n",
       "       [1],\n",
       "       [0],\n",
       "       [3],\n",
       "       [1],\n",
       "       [2],\n",
       "       [2],\n",
       "       [3],\n",
       "       [3],\n",
       "       [6],\n",
       "       [4],\n",
       "       [7],\n",
       "       [5],\n",
       "       [0],\n",
       "       [6],\n",
       "       [2],\n",
       "       [7],\n",
       "       [9],\n",
       "       [8],\n",
       "       [5],\n",
       "       [9],\n",
       "       [2],\n",
       "       [1],\n",
       "       [1],\n",
       "       [4],\n",
       "       [4],\n",
       "       [5],\n",
       "       [6],\n",
       "       [4],\n",
       "       [1],\n",
       "       [2],\n",
       "       [5],\n",
       "       [3],\n",
       "       [9],\n",
       "       [3],\n",
       "       [9],\n",
       "       [0],\n",
       "       [5],\n",
       "       [9],\n",
       "       [6],\n",
       "       [5],\n",
       "       [7],\n",
       "       [4],\n",
       "       [1],\n",
       "       [3],\n",
       "       [4],\n",
       "       [0],\n",
       "       [4],\n",
       "       [8],\n",
       "       [0],\n",
       "       [4],\n",
       "       [3],\n",
       "       [6],\n",
       "       [8],\n",
       "       [7],\n",
       "       [6],\n",
       "       [0],\n",
       "       [9],\n",
       "       [7],\n",
       "       [5],\n",
       "       [7],\n",
       "       [2],\n",
       "       [1],\n",
       "       [1],\n",
       "       [6],\n",
       "       [8],\n",
       "       [9],\n",
       "       [4],\n",
       "       [1],\n",
       "       [5],\n",
       "       [2],\n",
       "       [2],\n",
       "       [9],\n",
       "       [0],\n",
       "       [3],\n",
       "       [9],\n",
       "       [6],\n",
       "       [7],\n",
       "       [2],\n",
       "       [0],\n",
       "       [3],\n",
       "       [5],\n",
       "       [4],\n",
       "       [3],\n",
       "       [6],\n",
       "       [5],\n",
       "       [8],\n",
       "       [9],\n",
       "       [5],\n",
       "       [4],\n",
       "       [7],\n",
       "       [4],\n",
       "       [2],\n",
       "       [7],\n",
       "       [3],\n",
       "       [4],\n",
       "       [8],\n",
       "       [9],\n",
       "       [1],\n",
       "       [9],\n",
       "       [2],\n",
       "       [8],\n",
       "       [7],\n",
       "       [9],\n",
       "       [1],\n",
       "       [8],\n",
       "       [7],\n",
       "       [4],\n",
       "       [1],\n",
       "       [3],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [2],\n",
       "       [3],\n",
       "       [9],\n",
       "       [4],\n",
       "       [9],\n",
       "       [2],\n",
       "       [1],\n",
       "       [6],\n",
       "       [8],\n",
       "       [4],\n",
       "       [7],\n",
       "       [7],\n",
       "       [4],\n",
       "       [4],\n",
       "       [9],\n",
       "       [2],\n",
       "       [5],\n",
       "       [7],\n",
       "       [2],\n",
       "       [4],\n",
       "       [4],\n",
       "       [2],\n",
       "       [1],\n",
       "       [9],\n",
       "       [7],\n",
       "       [2],\n",
       "       [8],\n",
       "       [7],\n",
       "       [6],\n",
       "       [9],\n",
       "       [2],\n",
       "       [2],\n",
       "       [3],\n",
       "       [8],\n",
       "       [1],\n",
       "       [6],\n",
       "       [5],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [2],\n",
       "       [6],\n",
       "       [4],\n",
       "       [5],\n",
       "       [8],\n",
       "       [3],\n",
       "       [1],\n",
       "       [5],\n",
       "       [1],\n",
       "       [9],\n",
       "       [2],\n",
       "       [7],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [8],\n",
       "       [1],\n",
       "       [5],\n",
       "       [8],\n",
       "       [9],\n",
       "       [5],\n",
       "       [6],\n",
       "       [7],\n",
       "       [9],\n",
       "       [9],\n",
       "       [3],\n",
       "       [7],\n",
       "       [0],\n",
       "       [9],\n",
       "       [0],\n",
       "       [6],\n",
       "       [6],\n",
       "       [2],\n",
       "       [3],\n",
       "       [9],\n",
       "       [0],\n",
       "       [7],\n",
       "       [5],\n",
       "       [4],\n",
       "       [8],\n",
       "       [0],\n",
       "       [9],\n",
       "       [4],\n",
       "       [1],\n",
       "       [2],\n",
       "       [8],\n",
       "       [7],\n",
       "       [1],\n",
       "       [2],\n",
       "       [6],\n",
       "       [1],\n",
       "       [0],\n",
       "       [3],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [8],\n",
       "       [2],\n",
       "       [0],\n",
       "       [3],\n",
       "       [9],\n",
       "       [4],\n",
       "       [0],\n",
       "       [5],\n",
       "       [0],\n",
       "       [6],\n",
       "       [1],\n",
       "       [7],\n",
       "       [7],\n",
       "       [8],\n",
       "       [1],\n",
       "       [9],\n",
       "       [2],\n",
       "       [0],\n",
       "       [5],\n",
       "       [1],\n",
       "       [2],\n",
       "       [2],\n",
       "       [7],\n",
       "       [3],\n",
       "       [5],\n",
       "       [4],\n",
       "       [9],\n",
       "       [7],\n",
       "       [1],\n",
       "       [8],\n",
       "       [3],\n",
       "       [9],\n",
       "       [6],\n",
       "       [0],\n",
       "       [3],\n",
       "       [1],\n",
       "       [1],\n",
       "       [2],\n",
       "       [6],\n",
       "       [3],\n",
       "       [5],\n",
       "       [7],\n",
       "       [6],\n",
       "       [8],\n",
       "       [3]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "data=pd.read_csv('/home/sachin/Downloads/Datasets/mnist_train.csv')\n",
    "\n",
    "\n",
    "x=data.drop(data.columns[0], axis=1)\n",
    "y=data[data.columns[0]]\n",
    "X_train = np.array(x.values)\n",
    "Y_train = np.array(y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "j=np.array(X_train[3]).reshape(28,28)\n",
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(j,cmap='gray_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_single_step(a_slice_prev, W):\n",
    "    s = W * a_slice_prev \n",
    "    Z = np.sum(s)\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_pad(x,p):\n",
    "    res=np.pad(x, ((p,p),(p,p)), 'constant', constant_values=0)\n",
    "    return res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolution2d(img, f1,p,s):\n",
    "    m, n = f1.shape\n",
    "    a, b = img.shape\n",
    "    r=int(((a-m+2*p)/s)+1)\n",
    "    c=int(((b-n+2*p)/s)+1)\n",
    "    #print \"before padding :\",data.shape\n",
    "    data=zero_pad(img,p)\n",
    "    #print \"after padding :\",data.shape\n",
    "    res=np.zeros(shape=(r,c))\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            x=i+m\n",
    "            y=j+n\n",
    "            a_slice_prev=img[i:x,j:y]\n",
    "            res[i][j]= conv_single_step(a_slice_prev,f1)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLu(z):\n",
    "    y=np.maximum(0,z)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolution_data(X_train,f1,pad,stride):\n",
    "    n=X_train.shape[0]\n",
    "    res=[]\n",
    "    for i in range(n):\n",
    "        j=np.array(X_train[i]).reshape(28,28)\n",
    "        c_j=convolution2d(j,f1,pad,stride)\n",
    "        c_j=np.ravel(c_j)\n",
    "        res.append(c_j)\n",
    "    res=np.array(res)\n",
    "    return res\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    exps = np.exp(X - np.max(X))\n",
    "    return exps / np.sum(exps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc(x,W,b):\n",
    "    res=np.dot(W,x)+b\n",
    "    \n",
    "    #res=res/1000\n",
    "    return res\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(X_train,Y_train,f1,pad,stride,W,b):\n",
    "    \n",
    "    cnvld_x=convolution_data(X_train,f1,pad,stride)\n",
    "    relu_x=ReLu(cnvld_x)\n",
    "    \n",
    "    #print \"shape after relu\",relu_x.shape\n",
    "    relu_x=relu_x.T\n",
    "    #print \"shape after relu transpose\",relu_x.shape\n",
    "    #W=np.random.randint(5,size=(10,relu_x.shape[0]))\n",
    "    #b=np.random.randint(1,size=(10,relu_x.shape[1]))\n",
    "    \n",
    "  \n",
    "    print()\"shape of w\",W.shape)\n",
    "    print (\"shape of b\",b.shape)\n",
    "    output=fc(relu_x,W,b)\n",
    "    #print \"shape of output\", output.shape\n",
    "    prob=softmax(output)\n",
    "    #prob=prob.max(axis=0)\n",
    "    #print \"shape ofy_prob\", prob.shape\n",
    "    loss=calculate_loss(Y_train,prob)\n",
    "    cache=(X_train,cnvld_x,relu_x,output,prob,f1,W,b)\n",
    "    \n",
    "    return cache,loss\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(y,y_prob):\n",
    "    loss_train=0\n",
    "    y=y.reshape(y.shape[0],1)\n",
    "    y_prob=y_prob.T\n",
    "    m = y.shape[0]\n",
    "    for i in np.arange(m):\n",
    "        loss_train = loss_train+(np.log(y_prob[i,y[i]]))\n",
    "    loss_train = loss_train/m\n",
    "    return -1*loss_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dl/da2=da2\n",
    "def der_loss(a2,y):\n",
    "    return np.divide(1,a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#da2/dz2=dz2\n",
    "def der_act2(a2):\n",
    "    return np.multiply(a2,np.subtract(1,a2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dz2/da1=da1\n",
    "def der_z2(W2,z2):\n",
    "    return W2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#derivative of relu\n",
    "def der_a1(z1):\n",
    "    return z1*(z1>0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(X,W):\n",
    "    m,n=X.shape\n",
    "    a,b=W.shape\n",
    "    r=m-a+1\n",
    "    c=n-b+1\n",
    "    #res=np.zeros(shape=(a*2,a*2))\n",
    "    res=np.ndarray(shape=(r,c),dtype=type(np.zeros([a,b])))\n",
    "    #res = np.empty(shape=(r,c))\n",
    "    \n",
    "    #print m,n,a,b,r,c\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            e_r=i+a\n",
    "            e_c=j+b\n",
    "            temp=X[i:e_r,j:e_c]\n",
    "            res[i][j]=temp\n",
    "    dw1_temp=np.zeros([3,3])\n",
    "    #print \"res\",res[0]\n",
    "    for i in range(res.shape[1]):\n",
    "        for k in range(res.shape[0]):\n",
    "            dw1_temp=dw1_temp+res[i][k]\n",
    "    \n",
    "    \n",
    "    return dw1_temp\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_prop(cache,y,alpha):\n",
    "    X,z1,a1,z2,a2,W1,W2,b=cache\n",
    "    \n",
    "    \n",
    "    #print \"shape of z1\",z1.shape\n",
    "    print(y.shape)\n",
    "    print(a2.shape)\n",
    "    #dl/da2=da2(just the notation)\n",
    "    da2=der_loss(a2,y)\n",
    "    \n",
    "    #da2/dz2=dz2\n",
    "    da2_by_dz2=der_act2(a2)\n",
    "    \n",
    "      \n",
    "    #dl/dw=dw (for updating the weights)\n",
    "    #print \"shape of da2\",da2.shape\n",
    "    #print \"shape of da2_by_dz2\",da2_by_dz2.shape\n",
    "    #print \"shape of a1\", a1.shape\n",
    "    dz2=np.multiply(da2,da2_by_dz2)\n",
    "    dW2=np.dot(dz2,a1.T)/X.shape[0]\n",
    "    #print \"shape of dw2\",dW2.shape\n",
    "    #print \"shape of dz2\",dz2.shape\n",
    "    \n",
    "    #dl/db=db\n",
    "    db=np.sum(da2*da2_by_dz2,axis=1,keepdims=True)/X.shape[0]\n",
    "    \n",
    "    #print \"shape of db\",db.shape\n",
    "    \n",
    "    \n",
    "    \n",
    "    #calculating dw1\n",
    "    #dz2/da1=da1\n",
    "    dz2_by_da1=der_z2(W2,z2)\n",
    "    #print \"shape of dz2_by_da1\",dz2_by_da1.shape\n",
    "    #print \"unique values dz2_by_da1\",np.unique(dz2_by_da1)\n",
    "    #print \"unique values of dz2\", np.unique(dz2)\n",
    "\n",
    "    da1_by_dz1=der_a1(z1)\n",
    "    #print \"shape of da1_by_dz1\",da1_by_dz1.shape\n",
    "    \n",
    "    \n",
    "    \n",
    "    #dz1/dw1\n",
    "    dz1_by_dw1=X\n",
    "    #print \"shape of dz1_by_dw1\",dz1_by_dw1.shape\n",
    "    #dl/dw1\n",
    "    #dw1=np.dot(dz2.T,dz2_by_da1)*da1_by_dz1\n",
    "    dw1_temp=np.dot(dz2.T,dz2_by_da1)\n",
    "    #print \"shape of dw1\",dw1.shape\n",
    "    #print \"unique values\",np.unique(dw1)\n",
    "    res_updated_filter=np.zeros([3,3])\n",
    "    for i in range(X.shape[0]):\n",
    "        res_updated_filter=res_updated_filter+func(dw1_temp[i].reshape(26,26),W1)\n",
    "        \n",
    "            \n",
    "    #print \"unique values in res_updated filet\",np.unique(res_updated_filter)\n",
    "    dw1=res_updated_filter\n",
    "   \n",
    "    W1=W1-alpha*dw1\n",
    "    W2=W2=alpha*dW2\n",
    "    b=b-alpha*db\n",
    "    return W1,W2,b\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=data.drop(data.columns[0], axis=1)\n",
    "y=data[data.columns[0]]\n",
    "X_train = np.array(x.values)\n",
    "Y_train = np.array(y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train[:50000]\n",
    "Y_train=Y_train[:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1=np.random.normal(0,.1,(3,3))\n",
    "W2=np.random.normal(0,.1,(10,676))\n",
    "b=np.random.normal(0,.1,(10,X_train_batch.shape[0]))\n",
    "alpha=0.000001\n",
    "loss_function=[]\n",
    "\n",
    "def batch_test(X_train_batch,Y_train_batch,W1,W2,b):\n",
    "    for i in range(20):\n",
    "        cache,loss=forward_prop(X_train_batch,Y_train_batch,W1,0,1,W2,b)\n",
    "        print(\"loss\",loss)\n",
    "        W1,W2,b=backward_prop(cache,Y_train_batch,alpha)\n",
    "        acc=pred_y(X_test,Y_test,W1,W2,b)\n",
    "        print(\"accuracy\",acc)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 1)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 676)\n",
      "(500, 676)\n",
      "(676, 500)\n",
      "(10, 500)\n"
     ]
    }
   ],
   "source": [
    "acc=pred_y(X_test,Y_test,W1,W2,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_y(X_test,y_test,W1,W2,b):\n",
    "        \n",
    "    cnvld_x=convolution_data(X_test,W1,0,1)\n",
    "    print(cnvld_x.shape)\n",
    "    \n",
    "    relu_x=ReLu(cnvld_x)\n",
    "    print(relu_x.shape)\n",
    "    \n",
    "    relu_x=relu_x.T\n",
    "    print(relu_x.shape)\n",
    "    \n",
    "    output=fc(relu_x,W2,b)\n",
    "    print(output.shape)\n",
    "    \n",
    "    prob=softmax(output)\n",
    "    row_index = np.argmax(prob, axis=0)\n",
    "    count=0\n",
    "    for i in range(X_test.shape[0]):\n",
    "        if row_index[i]==y_test[i]:\n",
    "            count+=1\n",
    "    acc=float(count)/500\n",
    "            \n",
    "    return acc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1=pd.read_csv('/home/sachin/Downloads/Datasets/mnist_test.csv')\n",
    "x=data1.drop(data1.columns[0], axis=1)\n",
    "y=data1[data1.columns[0]]\n",
    "X_test = np.array(x.values)\n",
    "Y_test = np.array(y.values)\n",
    "X_test=X_train[:500]\n",
    "Y_test=Y_train[:500]\n",
    "Y_test=Y_test.reshape(500,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 784)\n",
      "loss [346.27648439]\n",
      "(500, 1)\n",
      "(10, 500)\n",
      "(500, 676)\n",
      "(500, 676)\n",
      "(676, 500)\n",
      "(10, 500)\n",
      "accuracy 0.11\n",
      "loss [8.52822986]\n",
      "(500, 1)\n",
      "(10, 500)\n",
      "(500, 676)\n",
      "(500, 676)\n",
      "(676, 500)\n",
      "(10, 500)\n",
      "accuracy 0.11\n",
      "loss [8.52876242]\n",
      "(500, 1)\n",
      "(10, 500)\n",
      "(500, 676)\n",
      "(500, 676)\n",
      "(676, 500)\n",
      "(10, 500)\n",
      "accuracy 0.11\n",
      "loss [8.52874673]\n",
      "(500, 1)\n",
      "(10, 500)\n",
      "(500, 676)\n",
      "(500, 676)\n",
      "(676, 500)\n",
      "(10, 500)\n",
      "accuracy 0.11\n",
      "loss [8.52873056]\n",
      "(500, 1)\n",
      "(10, 500)\n",
      "(500, 676)\n",
      "(500, 676)\n",
      "(676, 500)\n",
      "(10, 500)\n",
      "accuracy 0.11\n",
      "loss [8.52871445]\n",
      "(500, 1)\n",
      "(10, 500)\n",
      "(500, 676)\n",
      "(500, 676)\n",
      "(676, 500)\n",
      "(10, 500)\n",
      "accuracy 0.11\n",
      "loss [8.52869841]\n",
      "(500, 1)\n",
      "(10, 500)\n",
      "(500, 676)\n",
      "(500, 676)\n",
      "(676, 500)\n",
      "(10, 500)\n",
      "accuracy 0.11\n",
      "loss [8.52868244]\n",
      "(500, 1)\n",
      "(10, 500)\n",
      "(500, 676)\n",
      "(500, 676)\n",
      "(676, 500)\n",
      "(10, 500)\n",
      "accuracy 0.11\n",
      "loss [8.52866655]\n",
      "(500, 1)\n",
      "(10, 500)\n",
      "(500, 676)\n",
      "(500, 676)\n",
      "(676, 500)\n",
      "(10, 500)\n",
      "accuracy 0.11\n",
      "loss [8.52865072]\n",
      "(500, 1)\n",
      "(10, 500)\n",
      "(500, 676)\n",
      "(500, 676)\n",
      "(676, 500)\n",
      "(10, 500)\n",
      "accuracy 0.11\n",
      "loss [8.52863496]\n",
      "(500, 1)\n",
      "(10, 500)\n",
      "(500, 676)\n",
      "(500, 676)\n",
      "(676, 500)\n",
      "(10, 500)\n",
      "accuracy 0.11\n",
      "loss [8.52861927]\n",
      "(500, 1)\n",
      "(10, 500)\n",
      "(500, 676)\n",
      "(500, 676)\n",
      "(676, 500)\n",
      "(10, 500)\n",
      "accuracy 0.11\n",
      "loss [8.52860365]\n",
      "(500, 1)\n",
      "(10, 500)\n",
      "(500, 676)\n",
      "(500, 676)\n",
      "(676, 500)\n",
      "(10, 500)\n",
      "accuracy 0.11\n",
      "loss [8.5285881]\n",
      "(500, 1)\n",
      "(10, 500)\n",
      "(500, 676)\n",
      "(500, 676)\n",
      "(676, 500)\n",
      "(10, 500)\n",
      "accuracy 0.11\n",
      "loss [8.52857262]\n",
      "(500, 1)\n",
      "(10, 500)\n",
      "(500, 676)\n",
      "(500, 676)\n",
      "(676, 500)\n",
      "(10, 500)\n",
      "accuracy 0.11\n",
      "loss [8.52855721]\n",
      "(500, 1)\n",
      "(10, 500)\n",
      "(500, 676)\n",
      "(500, 676)\n",
      "(676, 500)\n",
      "(10, 500)\n",
      "accuracy 0.11\n",
      "loss [8.52854187]\n",
      "(500, 1)\n",
      "(10, 500)\n",
      "(500, 676)\n",
      "(500, 676)\n",
      "(676, 500)\n",
      "(10, 500)\n",
      "accuracy 0.11\n",
      "loss [8.5285266]\n",
      "(500, 1)\n",
      "(10, 500)\n",
      "(500, 676)\n",
      "(500, 676)\n",
      "(676, 500)\n",
      "(10, 500)\n",
      "accuracy 0.11\n",
      "loss [8.52851139]\n",
      "(500, 1)\n",
      "(10, 500)\n",
      "(500, 676)\n",
      "(500, 676)\n",
      "(676, 500)\n",
      "(10, 500)\n",
      "accuracy 0.11\n",
      "loss [8.52849625]\n",
      "(500, 1)\n",
      "(10, 500)\n",
      "(500, 676)\n",
      "(500, 676)\n",
      "(676, 500)\n",
      "(10, 500)\n",
      "accuracy 0.11\n",
      "(500, 784)\n",
      "loss [353.30634096]\n",
      "(500, 1)\n",
      "(10, 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sachin/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in true_divide\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/sachin/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:21: RuntimeWarning: invalid value encountered in add\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 676)\n",
      "(500, 676)\n",
      "(676, 500)\n",
      "(10, 500)\n",
      "accuracy 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sachin/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:26: RuntimeWarning: invalid value encountered in reduce\n",
      "  return umr_maximum(a, axis, None, out, keepdims)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss [nan]\n",
      "(500, 1)\n",
      "(10, 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sachin/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in greater\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 676)\n",
      "(500, 676)\n",
      "(676, 500)\n",
      "(10, 500)\n",
      "accuracy 0.1\n",
      "loss [nan]\n",
      "(500, 1)\n",
      "(10, 500)\n",
      "(500, 676)\n",
      "(500, 676)\n",
      "(676, 500)\n",
      "(10, 500)\n",
      "accuracy 0.1\n",
      "loss [nan]\n",
      "(500, 1)\n",
      "(10, 500)\n",
      "(500, 676)\n",
      "(500, 676)\n",
      "(676, 500)\n",
      "(10, 500)\n",
      "accuracy 0.1\n",
      "loss [nan]\n",
      "(500, 1)\n",
      "(10, 500)\n",
      "(500, 676)\n",
      "(500, 676)\n",
      "(676, 500)\n",
      "(10, 500)\n",
      "accuracy 0.1\n",
      "loss [nan]\n",
      "(500, 1)\n",
      "(10, 500)\n",
      "(500, 676)\n",
      "(500, 676)\n",
      "(676, 500)\n",
      "(10, 500)\n",
      "accuracy 0.1\n",
      "loss [nan]\n",
      "(500, 1)\n",
      "(10, 500)\n",
      "(500, 676)\n",
      "(500, 676)\n",
      "(676, 500)\n",
      "(10, 500)\n",
      "accuracy 0.1\n",
      "loss [nan]\n",
      "(500, 1)\n",
      "(10, 500)\n",
      "(500, 676)\n",
      "(500, 676)\n",
      "(676, 500)\n",
      "(10, 500)\n",
      "accuracy 0.1\n",
      "loss [nan]\n",
      "(500, 1)\n",
      "(10, 500)\n",
      "(500, 676)\n",
      "(500, 676)\n",
      "(676, 500)\n",
      "(10, 500)\n",
      "accuracy 0.1\n",
      "loss [nan]\n",
      "(500, 1)\n",
      "(10, 500)\n",
      "(500, 676)\n",
      "(500, 676)\n",
      "(676, 500)\n",
      "(10, 500)\n",
      "accuracy 0.1\n",
      "loss [nan]\n",
      "(500, 1)\n",
      "(10, 500)\n",
      "(500, 676)\n",
      "(500, 676)\n",
      "(676, 500)\n",
      "(10, 500)\n",
      "accuracy 0.1\n",
      "loss [nan]\n",
      "(500, 1)\n",
      "(10, 500)\n",
      "(500, 676)\n",
      "(500, 676)\n",
      "(676, 500)\n",
      "(10, 500)\n",
      "accuracy 0.1\n",
      "loss [nan]\n",
      "(500, 1)\n",
      "(10, 500)\n",
      "(500, 676)\n",
      "(500, 676)\n",
      "(676, 500)\n",
      "(10, 500)\n",
      "accuracy 0.1\n",
      "loss [nan]\n",
      "(500, 1)\n",
      "(10, 500)\n",
      "(500, 676)\n",
      "(500, 676)\n",
      "(676, 500)\n",
      "(10, 500)\n",
      "accuracy 0.1\n",
      "loss [nan]\n",
      "(500, 1)\n",
      "(10, 500)\n",
      "(500, 676)\n",
      "(500, 676)\n",
      "(676, 500)\n",
      "(10, 500)\n",
      "accuracy 0.1\n",
      "loss [nan]\n",
      "(500, 1)\n",
      "(10, 500)\n",
      "(500, 676)\n",
      "(500, 676)\n",
      "(676, 500)\n",
      "(10, 500)\n",
      "accuracy 0.1\n",
      "loss [nan]\n",
      "(500, 1)\n",
      "(10, 500)\n",
      "(500, 676)\n",
      "(500, 676)\n",
      "(676, 500)\n",
      "(10, 500)\n",
      "accuracy 0.1\n",
      "loss [nan]\n",
      "(500, 1)\n",
      "(10, 500)\n",
      "(500, 676)\n",
      "(500, 676)\n",
      "(676, 500)\n",
      "(10, 500)\n",
      "accuracy 0.1\n",
      "loss [nan]\n",
      "(500, 1)\n",
      "(10, 500)\n",
      "(500, 676)\n",
      "(500, 676)\n",
      "(676, 500)\n",
      "(10, 500)\n",
      "accuracy 0.1\n",
      "loss [nan]\n",
      "(500, 1)\n",
      "(10, 500)\n",
      "(500, 676)\n",
      "(500, 676)\n",
      "(676, 500)\n",
      "(10, 500)\n",
      "accuracy 0.1\n",
      "(500, 784)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sachin/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:7: RuntimeWarning: divide by zero encountered in log\n",
      "  import sys\n",
      "/home/sachin/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/sachin/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in multiply\n",
      "/home/sachin/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:25: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss [inf]\n",
      "(500, 1)\n",
      "(10, 500)\n",
      "(500, 676)\n",
      "(500, 676)\n",
      "(676, 500)\n",
      "(10, 500)\n",
      "accuracy 0.1\n",
      "loss [nan]\n",
      "(500, 1)\n",
      "(10, 500)\n",
      "(500, 676)\n",
      "(500, 676)\n",
      "(676, 500)\n",
      "(10, 500)\n",
      "accuracy 0.1\n",
      "loss [nan]\n",
      "(500, 1)\n",
      "(10, 500)\n",
      "(500, 676)\n",
      "(500, 676)\n",
      "(676, 500)\n",
      "(10, 500)\n",
      "accuracy 0.1\n",
      "loss [nan]\n",
      "(500, 1)\n",
      "(10, 500)\n",
      "(500, 676)\n",
      "(500, 676)\n",
      "(676, 500)\n",
      "(10, 500)\n",
      "accuracy 0.1\n",
      "loss [nan]\n",
      "(500, 1)\n",
      "(10, 500)\n",
      "(500, 676)\n",
      "(500, 676)\n",
      "(676, 500)\n",
      "(10, 500)\n",
      "accuracy 0.1\n",
      "loss [nan]\n",
      "(500, 1)\n",
      "(10, 500)\n",
      "(500, 676)\n",
      "(500, 676)\n",
      "(676, 500)\n",
      "(10, 500)\n",
      "accuracy 0.1\n",
      "loss [nan]\n",
      "(500, 1)\n",
      "(10, 500)\n",
      "(500, 676)\n",
      "(500, 676)\n",
      "(676, 500)\n",
      "(10, 500)\n",
      "accuracy 0.1\n",
      "loss [nan]\n",
      "(500, 1)\n",
      "(10, 500)\n",
      "(500, 676)\n",
      "(500, 676)\n",
      "(676, 500)\n",
      "(10, 500)\n",
      "accuracy 0.1\n",
      "loss [nan]\n",
      "(500, 1)\n",
      "(10, 500)\n",
      "(500, 676)\n",
      "(500, 676)\n",
      "(676, 500)\n",
      "(10, 500)\n",
      "accuracy 0.1\n",
      "loss [nan]\n",
      "(500, 1)\n",
      "(10, 500)\n",
      "(500, 676)\n",
      "(500, 676)\n",
      "(676, 500)\n",
      "(10, 500)\n",
      "accuracy 0.1\n",
      "loss [nan]\n",
      "(500, 1)\n",
      "(10, 500)\n",
      "(500, 676)\n",
      "(500, 676)\n",
      "(676, 500)\n",
      "(10, 500)\n",
      "accuracy 0.1\n",
      "loss [nan]\n",
      "(500, 1)\n",
      "(10, 500)\n",
      "(500, 676)\n",
      "(500, 676)\n",
      "(676, 500)\n",
      "(10, 500)\n",
      "accuracy 0.1\n",
      "loss [nan]\n",
      "(500, 1)\n",
      "(10, 500)\n",
      "(500, 676)\n",
      "(500, 676)\n",
      "(676, 500)\n",
      "(10, 500)\n",
      "accuracy 0.1\n",
      "loss [nan]\n",
      "(500, 1)\n",
      "(10, 500)\n",
      "(500, 676)\n",
      "(500, 676)\n",
      "(676, 500)\n",
      "(10, 500)\n",
      "accuracy 0.1\n",
      "loss [nan]\n",
      "(500, 1)\n",
      "(10, 500)\n",
      "(500, 676)\n",
      "(500, 676)\n",
      "(676, 500)\n",
      "(10, 500)\n",
      "accuracy 0.1\n",
      "loss [nan]\n",
      "(500, 1)\n",
      "(10, 500)\n",
      "(500, 676)\n",
      "(500, 676)\n",
      "(676, 500)\n",
      "(10, 500)\n",
      "accuracy 0.1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-e8c09ce0c38c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mY_train_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mbatch_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_train_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mW1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mW2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-83-b3797ebe1be7>\u001b[0m in \u001b[0;36mbatch_test\u001b[0;34m(X_train_batch, Y_train_batch, W1, W2, b)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbatch_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_train_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mW1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mW2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mcache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforward_prop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_train_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mW1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mW2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mW1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mW2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbackward_prop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_train_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-18a03cb3ad9b>\u001b[0m in \u001b[0;36mforward_prop\u001b[0;34m(X_train, Y_train, f1, pad, stride, W, b)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mforward_prop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m#print \"initial input shape\",X_train.shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mcnvld_x\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvolution_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;31m#print \"shape after convolution \",cnvld_x.shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#print \"convolved_x\",cnvld_x\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-180bad14c262>\u001b[0m in \u001b[0;36mconvolution_data\u001b[0;34m(X_train, f1, pad, stride)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mc_j\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvolution2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mc_j\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_j\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_j\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-83a8e954b6f0>\u001b[0m in \u001b[0;36mconvolution2d\u001b[0;34m(img, f1, p, s)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mres\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_train=X_train[:50000]\n",
    "Y_train=Y_train[:50000]\n",
    "for batch in range(10):\n",
    "    b=np.random.normal(0,.1,(10,X_train_batch.shape[0]))\n",
    "    X_train_batch=X_train[batch*500:(batch+1)*500]\n",
    "    Y_train_batch=Y_train[batch*500:(batch+1)*500].reshape(500,1)\n",
    "    print(X_train_batch.shape)\n",
    "    batch_test(X_train_batch,Y_train_batch,W1,W2,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
